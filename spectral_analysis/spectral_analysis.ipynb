{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.io import wavfile\n",
    "import pyworld\n",
    "import pysptk\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "import numpy as np\n",
    "from nnmnkwii.preprocessing.alignment import DTWAligner\n",
    "from nnmnkwii.metrics import melcd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mfcc = 40\n",
    "NAME =  ...\n",
    "DATASET_PATH = f\"../data_generation/{NAME}/recordings\"\n",
    "INPUT_DIR = \"/joanna\"\n",
    "OUTPUT_DIR = \"/human\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "fs = 22050\n",
    "fftlen = pyworld.get_cheaptrick_fft_size(fs)\n",
    "alpha = pysptk.util.mcepalpha(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in sorted(os.listdir(DATASET_PATH + INPUT_DIR)):\n",
    "    path = os.path.join(DATASET_PATH + INPUT_DIR, file)\n",
    "    fs, data = wavfile.read(path)\n",
    "    inputs.append(data)\n",
    "\n",
    "for file in sorted(os.listdir(DATASET_PATH + OUTPUT_DIR)):\n",
    "    path = os.path.join(DATASET_PATH + OUTPUT_DIR, file)\n",
    "    fs, data = wavfile.read(path)\n",
    "    outputs.append(data)\n",
    "\n",
    "IPython.display.display(Audio(inputs[3], rate = fs))\n",
    "IPython.display.display(Audio(outputs[3], rate = fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data = data.astype(np.float64)\n",
    "    f0, sp, ap = pyworld.wav2world(data, fs)\n",
    "    mcc = pysptk.sp2mc(sp, order=n_mfcc, alpha = alpha)\n",
    "    return mcc, f0, ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocessed_inputs = []\n",
    "preprocessed_outputs = []\n",
    "\n",
    "for data in tqdm.tqdm(inputs, \"INPUTS\"):\n",
    "    preprocessed_inputs.append(preprocess(data))\n",
    "for data in tqdm.tqdm(outputs, \"OUTPUTS\"):\n",
    "    preprocessed_outputs.append(preprocess(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(mcc, f0, ap):\n",
    "    sp = pysptk.mc2sp(\n",
    "            mcc.astype(np.float64), alpha=alpha, fftlen=fftlen)\n",
    "    waveform = pyworld.synthesize(\n",
    "            f0, sp, ap, fs)\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test preprocess and decode\n",
    "mcc, f0, ap = preprocessed_inputs[3]\n",
    "waveform = decode(mcc, f0, ap)\n",
    "IPython.display.display(Audio(waveform, rate = fs))\n",
    "\n",
    "mcc, f0, ap = preprocessed_outputs[3]\n",
    "waveform = decode(mcc, f0, ap)\n",
    "IPython.display.display(Audio(waveform, rate = fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner = DTWAligner(verbose=0, dist=melcd)\n",
    "def align(X, Y):\n",
    "    X, Y = aligner.transform((np.array([X]), np.array([Y])))\n",
    "    return X[0], Y[0]\n",
    "\n",
    "aligned_inputs = []\n",
    "aligned_outputs = []\n",
    "for i in range(len(preprocessed_inputs)):\n",
    "    mcc_input, mcc_output = preprocessed_inputs[i][0], preprocessed_outputs[i][0]\n",
    "    aligned_input, aligned_output = align(mcc_input,mcc_output)\n",
    "    aligned_inputs.append(aligned_input)\n",
    "    aligned_outputs.append(aligned_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(preprocessed_inputs[3][0], aspect = 'auto')\n",
    "plt.show()\n",
    "plt.imshow(preprocessed_outputs[3][0], aspect = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(aligned_inputs[3], aspect = 'auto')\n",
    "plt.show()\n",
    "plt.imshow(aligned_outputs[3], aspect = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack(aligned_inputs)\n",
    "Y = np.vstack(aligned_outputs)\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "torch.save(X,f'X_{NAME}')\n",
    "torch.save(Y,f'Y_{NAME}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
